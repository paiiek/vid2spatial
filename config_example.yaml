# Example configuration for spatial audio pipeline
# Usage: python -m mmhoa.vid2spatial.run_demo --config config_example.yaml

# Input files
video_path: "path/to/video.mp4"
audio_path: "path/to/mono.wav"

# Optional precomputed data
trajectory_json: null  # or "path/to/trajectory.json"
air_foa_path: null  # or "path/to/foa_air.wav"
brir_left_path: null
brir_right_path: null

# Vision processing
vision:
  camera:
    fov_deg: 60.0
    sample_stride: 1

  tracking:
    method: "yolo"  # yolo, kcf, sam2
    class_name: "person"
    select_track_id: null
    init_bbox: null  # [x, y, w, h] for KCF
    fallback_center_if_no_bbox: false
    smooth_alpha: 0.2

  depth:
    backend: "auto"  # auto, midas, none
    use_adapter: false

  refinement:
    enabled: false
    method: "grabcut"  # grabcut, sam2
    sam_ckpt: null
    sam2_model_id: "facebook/sam2.1-hiera-base-plus"
    sam2_cfg: null
    sam2_ckpt: null

# Room acoustics
room:
  dimensions: [6.0, 5.0, 3.0]  # [Lx, Ly, Lz] in meters
  mic_position: [3.0, 2.5, 1.5]  # [mx, my, mz] in meters
  rt60: 0.6  # reverberation time in seconds
  backend: "auto"  # auto, pra, schroeder, none, visual, brir
  disabled: false

# Spatial audio rendering
spatial:
  # Angle smoothing
  angle_smooth_ms: 50.0
  max_deg_per_s: null  # null = no limit

  # Distance effects
  dist_gain_k: 1.0  # 0.0 = disabled, 1.0 = full effect
  dist_lpf_min_hz: 800.0
  dist_lpf_max_hz: 8000.0

# Occlusion handling
occlusion:
  enabled: false
  estimate: false  # estimate from video
  json_path: null  # or "path/to/occlusion.json"

# Reverb
reverb:
  enabled: false
  rt60: 0.6
  wet_min: 0.05
  wet_max: 0.35
  wet_occ_boost: 0.10

# Output files
output:
  foa_path: "output.foa.wav"
  stereo_path: null  # or "output.stereo.wav"
  binaural_path: null  # or "output.binaural.wav"

  binaural_config:
    mode: "crossfeed"  # crossfeed, sofa
    sofa_path: null  # required for mode=sofa

  trajectory_path: null  # or "trajectory.json"
